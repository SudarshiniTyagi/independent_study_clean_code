{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate p-value significance test for model selection. Model selection is important to choose which model works best on your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'diagnosis',\n",
       " 'radius_mean',\n",
       " 'texture_mean',\n",
       " 'perimeter_mean',\n",
       " 'area_mean',\n",
       " 'smoothness_mean',\n",
       " 'compactness_mean',\n",
       " 'concavity_mean',\n",
       " 'concave points_mean',\n",
       " 'symmetry_mean',\n",
       " 'fractal_dimension_mean',\n",
       " 'radius_se',\n",
       " 'texture_se',\n",
       " 'perimeter_se',\n",
       " 'area_se',\n",
       " 'smoothness_se',\n",
       " 'compactness_se',\n",
       " 'concavity_se',\n",
       " 'concave points_se',\n",
       " 'symmetry_se',\n",
       " 'fractal_dimension_se',\n",
       " 'radius_worst',\n",
       " 'texture_worst',\n",
       " 'perimeter_worst',\n",
       " 'area_worst',\n",
       " 'smoothness_worst',\n",
       " 'compactness_worst',\n",
       " 'concavity_worst',\n",
       " 'concave points_worst',\n",
       " 'symmetry_worst',\n",
       " 'fractal_dimension_worst',\n",
       " 'Unnamed: 32']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('breast_cancer_data.csv')\n",
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode:\n",
    "#\n",
    "# 1. Measure the difference between the two group means.  The difference in means is measured\n",
    "#    by (sum(grpA) / len(grpA)) - (sum(grpB) / len(grpB)).  \n",
    "#\n",
    "# 2. Set a counter to 0, this will count the number of times we get a difference\n",
    "#    between the means greater than or equal to the original(calculated in step 1).  \n",
    "#\n",
    "# 3. Do the following 10,000 times:\n",
    "#    a. Shuffle the original measurements.  To do this:\n",
    "#       i. put the values from all the groups into one array but remembering the start and end indexes of each group\n",
    "#       ii. shuffle the values in the array, effectively reassigning the values to different groups\n",
    "#    b. Measure the difference between the two group means, just as we did in step (1).\n",
    "#    c. If the difference from step (3b) is greater than or equal to the difference calculated in step 1, increment our counter \n",
    "#       from step (2). Note: if our original difference between the means were a negative value \n",
    "#       we would check for values less than or equal to that value.\n",
    "#\n",
    "# 4. counter / 10,000 equals the probability of getting our observed difference of two means greater than\n",
    "#    or equal to 12.97, if there is in fact no significant difference.\n",
    "\n",
    "import random\n",
    "\n",
    "def shuffle(grps):\n",
    "    num_grps = len(grps)\n",
    "    pool = []\n",
    "\n",
    "    # pool all values\n",
    "    for i in range(num_grps):\n",
    "        pool.extend(grps[i])\n",
    "    # mix them up\n",
    "    random.shuffle(pool)\n",
    "    # reassign to groups of same size as original groups\n",
    "    new_grps = []\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    for i in range(num_grps):\n",
    "        end_index = start_index + len(grps[i])\n",
    "        new_grps.append(pool[start_index:end_index])\n",
    "        start_index = end_index\n",
    "    return new_grps\n",
    "\n",
    "# subtracts group a mean from group b mean and returns result\n",
    "def meandiff(grpA, grpB):\n",
    "    return (sum(grpB) / float(len(grpB)) - sum(grpA) / float(len(grpA)))*100\n",
    "\n",
    "def p_test(samples):\n",
    "    a = 0\n",
    "    b = 1\n",
    "    observed_mean_diff = meandiff(samples[a], samples[b])\n",
    "    \n",
    "\n",
    "    count = 0\n",
    "    num_shuffles = 10000\n",
    "    for i in range(num_shuffles):\n",
    "        new_samples = shuffle(samples)\n",
    "        mean_diff = meandiff(new_samples[a], new_samples[b])\n",
    "        # if the observed difference is negative, look for differences that are smaller\n",
    "        # if the observed difference is positive, look for differences that are greater\n",
    "        if observed_mean_diff < 0 and mean_diff <= observed_mean_diff:\n",
    "            count = count + 1\n",
    "        elif observed_mean_diff >= 0 and mean_diff >= observed_mean_diff:\n",
    "            count = count + 1\n",
    "    ######################################\n",
    "    #\n",
    "    # Output\n",
    "    #\n",
    "    ######################################\n",
    "\n",
    "    print (\"Observed difference of two means: %.2f\" % observed_mean_diff)\n",
    "    print (count, \"out of\", num_shuffles, \"experiments had a difference of two means \", end=\"\")\n",
    "    if observed_mean_diff < 0:\n",
    "        print (\"less than or equal to \", end=\"\")\n",
    "    else:\n",
    "        print (\"greater than or equal to \", end=\" \")\n",
    "    print (\"%.2f\" % observed_mean_diff, \".\")\n",
    "    print (\"The chance of getting a difference of two means\", end=\" \")\n",
    "    if observed_mean_diff < 0:\n",
    "        print (\"less than or equal to \", end=\"\")\n",
    "    else:\n",
    "        print (\"greater than or equal to \", end=\"\")\n",
    "    print (\"%.2f\" % observed_mean_diff, \"is\", (count / float(num_shuffles)), \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As concluded in the last notebook, we only use features with `_mean` prefix. Hence, we remove all the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\"Unnamed: 32\",\"id\", \"perimeter_mean\", \"area_mean\",\n",
    "                       \"perimeter_se\", \"area_se\", \"radius_worst\", \n",
    "                       \"texture_worst\", \n",
    "                       \"perimeter_worst\", \n",
    "                       \"area_worst\", \n",
    "                       \"smoothness_worst\", \n",
    "                       \"compactness_worst\", \n",
    "                       \"concavity_worst\",\n",
    "                       \"concave points_mean\", \n",
    "                       \"symmetry_worst\", \n",
    "                       \"fractal_dimension_worst\",\n",
    "                       \"concavity_mean\",\n",
    "                       \"concavity_se\",\n",
    "                       \"concave points_worst\",\n",
    "                       \"concave points_se\",\n",
    "                       \"radius_se\",\n",
    "                       \"texture_se\",\n",
    "                       \"smoothness_se\",\n",
    "                       \"compactness_se\",\n",
    "                       \"symmetry_se\",\n",
    "                       \"fractal_dimension_se\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns_to_remove, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diagnosis',\n",
       " 'radius_mean',\n",
       " 'texture_mean',\n",
       " 'smoothness_mean',\n",
       " 'compactness_mean',\n",
       " 'symmetry_mean',\n",
       " 'fractal_dimension_mean']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required modules from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we generate labels, denoted by Y (diagnosis column) and the rest of the data will be denoted by X\n",
    "labels = df['diagnosis']\n",
    "Y = [1 if ele == \"M\" else 0 for ele in labels] #convert labels M and B to binary(1 and 0)\n",
    "df = df.drop('diagnosis', axis=1)#drop the diagnosis column from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "\n",
    "#splitting data into train and test\n",
    "#this is done to test on samples that the model has not been trained on, it leads to less bias\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  90.10989010989012\n",
      "Test accuracy:  90.35087719298247\n",
      "Train F-1 score:  85.43689320388349\n",
      "Test F-1 score:  84.93150684931507\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression().fit(X_train, Y_train)\n",
    "Y_predicted_lr_train = clf.predict(X_train)\n",
    "Y_predicted_lr = clf.predict(X_test)\n",
    "lr_accuracy = np.logical_xor(Y_predicted_lr, Y_test)\n",
    "lr_accuracy = np.logical_not(lr_accuracy)\n",
    "print(\"Train accuracy: \",clf.score(X_train, Y_train)*100)\n",
    "print(\"Test accuracy: \",clf.score(X_test, Y_test)*100)\n",
    "print(\"Train F-1 score: \", f1_score(Y_train, Y_predicted_lr_train)*100)\n",
    "print(\"Test F-1 score: \", f1_score(Y_test, Y_predicted_lr)*100)\n",
    "lr_accuracy_int = [1 if ele == True else 0 for ele in lr_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  100.0\n",
      "Test accuracy:  91.22807017543859\n",
      "Train F-1 score:  100.0\n",
      "Test F-1 score:  86.8421052631579\n"
     ]
    }
   ],
   "source": [
    "clf_gini = DecisionTreeClassifier(criterion=\"gini\").fit(X_train, Y_train)\n",
    "Y_predicted_dt_train = clf_gini.predict(X_train)\n",
    "Y_predicted_dt = clf_gini.predict(X_test)\n",
    "dt_accuracy = np.logical_xor(Y_predicted_dt, Y_test)\n",
    "dt_accuracy = np.logical_not(dt_accuracy)\n",
    "print(\"Train accuracy: \",clf_gini.score(X_train, Y_train)*100)\n",
    "print(\"Test accuracy: \",clf_gini.score(X_test, Y_test)*100)\n",
    "print(\"Train F-1 score: \", f1_score(Y_train, Y_predicted_dt_train)*100)\n",
    "print(\"Test F-1 score: \", f1_score(Y_test, Y_predicted_dt)*100)\n",
    "dt_accuracy_int = [1 if ele == True else 0 for ele in dt_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  100.0\n",
      "Test accuracy:  93.85964912280701\n",
      "Train F-1 score:  100.0\n",
      "Test F-1 score:  91.13924050632912\n"
     ]
    }
   ],
   "source": [
    "clf_random_forest = RandomForestClassifier().fit(X_train, Y_train)\n",
    "Y_predicted_rf_train = clf_random_forest.predict(X_train)\n",
    "Y_predicted_rf = clf_random_forest.predict(X_test)\n",
    "rf_accuracy = np.logical_xor(Y_predicted_rf, Y_test)\n",
    "rf_accuracy = np.logical_not(rf_accuracy)\n",
    "print(\"Train accuracy: \",clf_random_forest.score(X_train, Y_train)*100)\n",
    "print(\"Test accuracy: \",clf_random_forest.score(X_test, Y_test)*100)\n",
    "print(\"Train F-1 score: \", f1_score(Y_train, Y_predicted_rf_train)*100)\n",
    "print(\"Test F-1 score: \", f1_score(Y_test, Y_predicted_rf)*100)\n",
    "rf_accuracy_int = [1 if ele == True else 0 for ele in rf_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  90.98901098901099\n",
      "Test accuracy:  88.59649122807018\n",
      "Train F-1 score:  87.22741433021807\n",
      "Test F-1 score:  83.9506172839506\n"
     ]
    }
   ],
   "source": [
    "clf_svm = svm.SVC().fit(X_train, Y_train)\n",
    "Y_predicted_svm_train = clf_svm.predict(X_train)\n",
    "Y_predicted_svm = clf_svm.predict(X_test)\n",
    "svm_accuracy = np.logical_xor(Y_predicted_svm, Y_test)\n",
    "svm_accuracy = np.logical_not(svm_accuracy)\n",
    "print(\"Train accuracy: \",clf_svm.score(X_train, Y_train)*100)\n",
    "print(\"Test accuracy: \",clf_svm.score(X_test, Y_test)*100)\n",
    "print(\"Train F-1 score: \", f1_score(Y_train, Y_predicted_svm_train)*100)\n",
    "print(\"Test F-1 score: \", f1_score(Y_test, Y_predicted_svm)*100)\n",
    "svm_accuracy_int = [1 if ele == True else 0 for ele in svm_accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p_test: Logistic Regression vs Decision Trees(with Gini Index criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference of two means: 0.88\n",
      "4930 out of 10000 experiments had a difference of two means greater than or equal to  0.88 .\n",
      "The chance of getting a difference of two means greater than or equal to 0.88 is 0.493 .\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "samples.append(lr_accuracy_int)\n",
    "samples.append(dt_accuracy_int)\n",
    "p_test(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p_test: Random Forest vs SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference of two means: -5.26\n",
      "1221 out of 10000 experiments had a difference of two means less than or equal to -5.26 .\n",
      "The chance of getting a difference of two means less than or equal to -5.26 is 0.1221 .\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "samples.append(rf_accuracy_int)\n",
    "samples.append(svm_accuracy_int)\n",
    "p_test(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p_test: Logistic Regression vs Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference of two means: 3.51\n",
      "2333 out of 10000 experiments had a difference of two means greater than or equal to  3.51 .\n",
      "The chance of getting a difference of two means greater than or equal to 3.51 is 0.2333 .\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "samples.append(lr_accuracy_int)\n",
    "samples.append(rf_accuracy_int)\n",
    "p_test(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p_test: Logistic Regression vs SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference of two means: -1.75\n",
      "4157 out of 10000 experiments had a difference of two means less than or equal to -1.75 .\n",
      "The chance of getting a difference of two means less than or equal to -1.75 is 0.4157 .\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "samples.append(lr_accuracy_int)\n",
    "samples.append(svm_accuracy_int)\n",
    "p_test(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p_test: Decision Trees vs SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference of two means: -2.63\n",
      "3256 out of 10000 experiments had a difference of two means less than or equal to -2.63 .\n",
      "The chance of getting a difference of two means less than or equal to -2.63 is 0.3256 .\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "samples.append(dt_accuracy_int)\n",
    "samples.append(svm_accuracy_int)\n",
    "p_test(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p_test: Decision Trees vs Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference of two means: 2.63\n",
      "3068 out of 10000 experiments had a difference of two means greater than or equal to  2.63 .\n",
      "The chance of getting a difference of two means greater than or equal to 2.63 is 0.3068 .\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "samples.append(dt_accuracy_int)\n",
    "samples.append(rf_accuracy_int)\n",
    "p_test(samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
